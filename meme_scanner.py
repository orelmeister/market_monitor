#!/usr/bin/env python3
"""
meme_scanner.py â€” Meme Coin Scanner for DEX Monitoring

Monitors DEXs across Solana, Ethereum, and Base for new token launches.
Runs 24/7 independently of traditional market hours.

Features:
- New token detection via DexScreener API
- Token safety analysis via GoPlus
- Trending token monitoring
- Configurable alerts via Telegram
"""

import logging
import os
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Optional
import requests

from notifications import send_alert

logger = logging.getLogger("meme_scanner")

# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# API Endpoints (all free, no keys required for basic use)
DEXSCREENER_API = "https://api.dexscreener.com"
GOPLUS_API = "https://api.gopluslabs.io/api/v1"
GECKOTERMINAL_API = "https://api.geckoterminal.com/api/v2"

# Chain IDs for GoPlus
CHAIN_IDS = {
    "solana": "solana",
    "ethereum": "1",
    "base": "8453",
    "bsc": "56",
    "arbitrum": "42161",
}

# Minimum liquidity to consider for alerts (in USD)
# Set low to catch early - can filter more strictly in signal level
MIN_LIQUIDITY_USD = 1000
# Maximum token age for "new" tokens (in minutes)
MAX_NEW_TOKEN_AGE_MINUTES = 120  # 2 hours window for new tokens

# RPC URLs from environment
SOLANA_RPC = os.getenv("SOLANA_RPC_URL", "")
ETHEREUM_RPC = os.getenv("ETHEREUM_RPC_URL", "")
BASE_RPC = os.getenv("BASE_RPC_URL", "")


@dataclass
class TokenInfo:
    """Information about a detected token."""
    address: str
    chain: str
    name: str
    symbol: str
    price_usd: Optional[float]
    liquidity_usd: Optional[float]
    volume_24h: Optional[float]
    price_change_24h: Optional[float]
    pair_created_at: Optional[datetime]
    dex: str
    url: str
    safety_score: Optional[int] = None
    is_honeypot: Optional[bool] = None
    mint_revoked: Optional[bool] = None
    freeze_revoked: Optional[bool] = None


@dataclass
class MemeSignal:
    """Signal from meme coin scanner."""
    level: str  # HOT, WATCHLIST, WARNING, INFO
    name: str
    message: str
    token: Optional[TokenInfo] = None


# â”€â”€â”€ Tracked Tokens State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_seen_tokens: set = set()  # Track already seen tokens to avoid duplicate alerts
_watchlist: dict = {}  # Tokens we're watching

# â”€â”€â”€ DexScreener API Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_new_pairs(chain: str = "solana", limit: int = 50) -> list[dict]:
    """
    Fetch recently created pairs - uses GeckoTerminal as primary source.
    
    DexScreener's /token-profiles/latest/v1 doesn't include liquidity/price data,
    so we use GeckoTerminal for new pool discovery instead.
    
    Args:
        chain: Chain to query (solana, ethereum, base, bsc)
        limit: Maximum pairs to return
        
    Returns:
        List of pair data dictionaries
    """
    # GeckoTerminal has better data for new pools (includes liquidity, price, etc.)
    pairs = get_new_pairs_geckoterminal(chain, limit)
    
    if pairs:
        return pairs
    
    # Fallback: try DexScreener trending/boosted as backup
    try:
        logger.info(f"GeckoTerminal returned no pairs for {chain}, trying DexScreener boosted...")
        url = f"{DEXSCREENER_API}/token-boosts/latest/v1"
        resp = requests.get(url, timeout=10)
        
        if resp.status_code != 200:
            return []
        
        data = resp.json()
        
        # Filter by chain
        chain_pairs = [p for p in data if p.get("chainId", "").lower() == chain.lower()]
        
        return chain_pairs[:limit]
        
    except Exception as e:
        logger.warning(f"DexScreener boosted also failed: {e}")
        return []


def get_new_pairs_geckoterminal(chain: str = "solana", limit: int = 50) -> list[dict]:
    """
    Fallback: Get new pools from GeckoTerminal.
    """
    try:
        network_map = {
            "solana": "solana",
            "ethereum": "eth",
            "base": "base",
            "bsc": "bsc",
        }
        network = network_map.get(chain.lower(), chain)
        
        url = f"{GECKOTERMINAL_API}/networks/{network}/new_pools?include=base_token,quote_token"
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        
        pools = data.get("data", [])
        included = {item["id"]: item for item in data.get("included", [])}
        
        # Convert to DexScreener-like format for compatibility
        pairs = []
        for pool in pools[:limit]:
            attrs = pool.get("attributes", {})
            relationships = pool.get("relationships", {})
            
            # Get base token info from included data
            base_token_ref = relationships.get("base_token", {}).get("data", {})
            base_token_id = base_token_ref.get("id", "")
            base_token_data = included.get(base_token_id, {}).get("attributes", {})
            
            # Extract token details
            token_address = base_token_data.get("address", "")
            token_name = base_token_data.get("name", "Unknown")
            token_symbol = base_token_data.get("symbol", "???")
            
            # Get pool metrics
            liquidity = float(attrs.get("reserve_in_usd") or 0)
            price_usd = attrs.get("base_token_price_usd")
            
            # Skip only if totally invalid (no address or no symbol)
            # Allow unknown liquidity for brand new tokens
            if not token_address or token_symbol == "???" or token_symbol == "":
                continue
            
            # Parse creation time
            created_at = attrs.get("pool_created_at")
            pair_created_ms = None
            if created_at:
                try:
                    dt = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
                    pair_created_ms = int(dt.timestamp() * 1000)
                except:
                    pass
            
            pairs.append({
                "chainId": chain,
                "pairAddress": pool.get("id", "").split("_")[-1] if "_" in pool.get("id", "") else pool.get("id"),
                "baseToken": {
                    "address": token_address,
                    "name": token_name,
                    "symbol": token_symbol,
                },
                "priceUsd": price_usd,
                "liquidity": {"usd": liquidity},
                "volume": {"h24": float(attrs.get("volume_usd", {}).get("h24", 0) or 0)},
                "priceChange": {"h24": float(attrs.get("price_change_percentage", {}).get("h24", 0) or 0)},
                "pairCreatedAt": pair_created_ms,
                "dexId": attrs.get("dex_id", "unknown"),
                "url": f"https://www.geckoterminal.com/{network}/pools/{pool.get('id', '').split('_')[-1] if '_' in pool.get('id', '') else pool.get('id')}",
            })
        
        return pairs
        
    except Exception as e:
        logger.error(f"GeckoTerminal new pools failed: {e}")
        return []


def get_token_pairs(token_address: str) -> list[dict]:
    """
    Get all trading pairs for a specific token.
    
    Args:
        token_address: Token contract address
        
    Returns:
        List of pair data
    """
    try:
        url = f"{DEXSCREENER_API}/latest/dex/tokens/{token_address}"
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        return data.get("pairs", [])
    except Exception as e:
        logger.error(f"Failed to fetch token pairs: {e}")
        return []


def get_trending_tokens(chain: str = "solana") -> list[dict]:
    """
    Get trending tokens from GeckoTerminal.
    
    Args:
        chain: Chain to query
        
    Returns:
        List of trending token data
    """
    try:
        # Map chain names to GeckoTerminal network IDs
        network_map = {
            "solana": "solana",
            "ethereum": "eth",
            "base": "base",
            "bsc": "bsc",
            "arbitrum": "arbitrum",
        }
        network = network_map.get(chain.lower(), chain)
        
        url = f"{GECKOTERMINAL_API}/networks/{network}/trending_pools"
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        return data.get("data", [])
    except Exception as e:
        logger.error(f"Failed to fetch trending tokens: {e}")
        return []


# â”€â”€â”€ Token Safety Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_token_safety(token_address: str, chain: str = "ethereum") -> dict:
    """
    Check token safety using GoPlus Security API.
    
    Args:
        token_address: Token contract address
        chain: Chain name
        
    Returns:
        Safety analysis dict with scores and flags
    """
    try:
        chain_id = CHAIN_IDS.get(chain.lower(), "1")
        
        # GoPlus requires different endpoint for Solana
        if chain.lower() == "solana":
            url = f"{GOPLUS_API}/solana/token_security?contract_addresses={token_address}"
        else:
            url = f"{GOPLUS_API}/token_security/{chain_id}?contract_addresses={token_address}"
        
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        
        if data.get("code") != 1:
            return {"error": "API error", "safe": False}
        
        result = data.get("result", {}).get(token_address.lower(), {})
        
        # Parse safety flags
        safety = {
            "is_honeypot": result.get("is_honeypot") == "1",
            "is_mintable": result.get("is_mintable") == "1",
            "can_take_back_ownership": result.get("can_take_back_ownership") == "1",
            "owner_change_balance": result.get("owner_change_balance") == "1",
            "hidden_owner": result.get("hidden_owner") == "1",
            "selfdestruct": result.get("selfdestruct") == "1",
            "external_call": result.get("external_call") == "1",
            "buy_tax": float(result.get("buy_tax", 0) or 0),
            "sell_tax": float(result.get("sell_tax", 0) or 0),
            "holder_count": int(result.get("holder_count", 0) or 0),
            "lp_holder_count": int(result.get("lp_holder_count", 0) or 0),
            "is_open_source": result.get("is_open_source") == "1",
        }
        
        # Calculate safety score (0-100)
        score = 100
        if safety["is_honeypot"]:
            score = 0  # Automatic fail
        else:
            if safety["is_mintable"]:
                score -= 30
            if safety["can_take_back_ownership"]:
                score -= 20
            if safety["owner_change_balance"]:
                score -= 20
            if safety["hidden_owner"]:
                score -= 15
            if safety["buy_tax"] > 5:
                score -= min(20, safety["buy_tax"])
            if safety["sell_tax"] > 5:
                score -= min(20, safety["sell_tax"])
            if not safety["is_open_source"]:
                score -= 10
        
        safety["score"] = max(0, score)
        safety["safe"] = score >= 50 and not safety["is_honeypot"]
        
        return safety
        
    except Exception as e:
        logger.error(f"Failed to check token safety: {e}")
        return {"error": str(e), "safe": False, "score": 0}


# â”€â”€â”€ Token Info Parsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_pair_to_token(pair: dict) -> Optional[TokenInfo]:
    """
    Parse DexScreener pair data into TokenInfo.
    
    Args:
        pair: Raw pair data from DexScreener
        
    Returns:
        TokenInfo object or None if parsing fails
    """
    try:
        base_token = pair.get("baseToken", {})
        
        # Parse creation time
        created_at = None
        if pair.get("pairCreatedAt"):
            created_at = datetime.fromtimestamp(pair["pairCreatedAt"] / 1000)
        
        return TokenInfo(
            address=base_token.get("address", ""),
            chain=pair.get("chainId", ""),
            name=base_token.get("name", "Unknown"),
            symbol=base_token.get("symbol", "???"),
            price_usd=float(pair.get("priceUsd", 0) or 0),
            liquidity_usd=float(pair.get("liquidity", {}).get("usd", 0) or 0),
            volume_24h=float(pair.get("volume", {}).get("h24", 0) or 0),
            price_change_24h=float(pair.get("priceChange", {}).get("h24", 0) or 0),
            pair_created_at=created_at,
            dex=pair.get("dexId", ""),
            url=pair.get("url", f"https://dexscreener.com/{pair.get('chainId')}/{pair.get('pairAddress')}"),
        )
    except Exception as e:
        logger.error(f"Failed to parse pair: {e}")
        return None


# â”€â”€â”€ Main Scanner Jobs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scan_new_tokens(chains: list[str] = None) -> list[MemeSignal]:
    """
    Scan for newly created tokens across specified chains.
    
    Args:
        chains: List of chains to scan (default: solana, base, ethereum)
        
    Returns:
        List of MemeSignal objects for new tokens found
    """
    if chains is None:
        chains = ["solana", "base", "ethereum"]
    
    signals = []
    
    for chain in chains:
        logger.info(f"Scanning {chain} for new tokens...")
        
        pairs = get_new_pairs(chain)
        
        for pair in pairs:
            token = parse_pair_to_token(pair)
            if not token:
                continue
            
            # Skip invalid tokens
            if not token.address or len(token.address) < 10:
                continue
            if token.symbol in ("???", "UNKNOWN", ""):
                continue
            if token.name in ("Unknown", "UNKNOWN", ""):
                continue
            
            # Skip if already seen
            token_key = f"{token.chain}:{token.address}"
            if token_key in _seen_tokens:
                continue
            
            # Check if token is new enough
            if token.pair_created_at:
                age_minutes = (datetime.now() - token.pair_created_at).total_seconds() / 60
                if age_minutes > MAX_NEW_TOKEN_AGE_MINUTES:
                    continue
            
            # Allow tokens with unknown liquidity (brand new) but filter out confirmed low liquidity
            # None means just created, 0 means no liquidity added yet
            if token.liquidity_usd is not None and 0 < token.liquidity_usd < MIN_LIQUIDITY_USD:
                continue  # Has liquidity but too low - skip
            
            # Mark as seen
            _seen_tokens.add(token_key)
            
            # Check safety
            safety = check_token_safety(token.address, token.chain)
            token.safety_score = safety.get("score", 0)
            token.is_honeypot = safety.get("is_honeypot", None)
            token.mint_revoked = not safety.get("is_mintable", True)
            
            # Determine signal level based on available data
            liquidity = token.liquidity_usd or 0
            
            # Default to WATCHLIST for new tokens (we want to see them!)
            if token.safety_score < 40 or safety.get("is_honeypot"):
                level = "WARNING"
            elif token.safety_score >= 80 and liquidity >= 20000:
                level = "HOT"
            elif token.safety_score >= 60 and liquidity >= 5000:
                level = "HOT"
            elif liquidity >= 1000 or token.liquidity_usd is None:
                # Has decent liquidity OR brand new (unknown liquidity)
                level = "WATCHLIST"
            else:
                level = "INFO"
            
            # Create signal
            age_str = ""
            if token.pair_created_at:
                age_min = int((datetime.now() - token.pair_created_at).total_seconds() / 60)
                age_str = f" | Age: {age_min}min"
            
            # Format liquidity - handle None for brand new tokens
            if token.liquidity_usd is not None:
                liq_str = f"${token.liquidity_usd:,.0f}"
            else:
                liq_str = "â³ Pending (brand new!)"
            
            price_str = f"${token.price_usd:.8f}" if token.price_usd else "â³ Pending"
            vol_str = f"${token.volume_24h:,.0f}" if token.volume_24h else "N/A"
            
            message = f"""
ğŸ†• NEW TOKEN DETECTED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Token: ${token.symbol} ({token.name})
Chain: {token.chain.upper()}
DEX: {token.dex}

ğŸ“Š METRICS
Liquidity: {liq_str}
Price: {price_str}
24h Volume: {vol_str}{age_str}

ğŸ”’ SAFETY SCORE: {token.safety_score}/100
Honeypot: {'âŒ YES' if token.is_honeypot else 'âœ… No'}
Mintable: {'âš ï¸ Yes' if not token.mint_revoked else 'âœ… Revoked'}

ğŸ”— Contract: {token.address[:20]}...
ğŸ“ˆ {token.url}
"""
            
            signal = MemeSignal(
                level=level,
                name=f"new_token_{token.symbol}",
                message=message.strip(),
                token=token,
            )
            signals.append(signal)
            
            logger.info(f"[{level}] New token: ${token.symbol} on {token.chain} - Safety: {token.safety_score}")
    
    return signals


def scan_trending_tokens(chains: list[str] = None) -> list[MemeSignal]:
    """
    Scan for trending tokens.
    
    Args:
        chains: List of chains to scan
        
    Returns:
        List of MemeSignal objects for trending tokens
    """
    if chains is None:
        chains = ["solana", "base"]
    
    signals = []
    
    for chain in chains:
        logger.info(f"Checking trending on {chain}...")
        
        trending = get_trending_tokens(chain)
        
        for item in trending[:5]:  # Top 5 trending
            try:
                attrs = item.get("attributes", {})
                name = attrs.get("name", "Unknown")
                
                # Only alert on significant movers
                price_change = float(attrs.get("price_change_percentage", {}).get("h24", 0) or 0)
                if abs(price_change) < 50:  # Only >50% moves
                    continue
                
                level = "INFO"
                if price_change > 100:
                    level = "HOT"
                elif price_change > 50:
                    level = "WATCHLIST"
                
                message = f"""
ğŸ“ˆ TRENDING: {name}
Chain: {chain.upper()}
24h Change: {price_change:+.1f}%
"""
                
                signal = MemeSignal(
                    level=level,
                    name=f"trending_{chain}_{name[:10]}",
                    message=message.strip(),
                )
                signals.append(signal)
                
            except Exception as e:
                logger.error(f"Error parsing trending item: {e}")
    
    return signals


# â”€â”€â”€ Scheduled Job Entry Points â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def job_meme_scan() -> None:
    """
    Scheduled job: Scan for new meme coins.
    Should run every 1-2 minutes, 24/7.
    """
    logger.info("â•â•â• Running Meme Coin Scan â•â•â•")
    try:
        signals = scan_new_tokens()
        
        for signal in signals:
            if signal.level in ("HOT", "WARNING"):
                # Immediate alert for hot finds or warnings
                send_alert(
                    subject=f"MEME: {signal.level}",
                    body=signal.message,
                    level=signal.level,
                    alert_key=signal.name,
                )
            elif signal.level == "WATCHLIST":
                # Lower priority notification
                send_alert(
                    subject="MEME: Watchlist",
                    body=signal.message,
                    level="INFO",
                    alert_key=signal.name,
                )
        
        logger.info(f"Meme scan complete: {len(signals)} signals generated")
        
    except Exception as e:
        logger.error(f"Meme scan failed: {e}", exc_info=True)


def job_trending_scan() -> None:
    """
    Scheduled job: Check trending tokens.
    Should run every 5 minutes, 24/7.
    """
    logger.info("â•â•â• Running Trending Scan â•â•â•")
    try:
        signals = scan_trending_tokens()
        
        for signal in signals:
            if signal.level == "HOT":
                send_alert(
                    subject="TRENDING: Hot Mover",
                    body=signal.message,
                    level="INFO",
                    alert_key=signal.name,
                )
        
        logger.info(f"Trending scan complete: {len(signals)} signals")
        
    except Exception as e:
        logger.error(f"Trending scan failed: {e}", exc_info=True)


# â”€â”€â”€ Direct Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("Testing meme scanner...")
    print()
    
    # Test new token scan
    print("=== Scanning for new tokens ===")
    signals = scan_new_tokens(["solana"])
    for sig in signals[:3]:  # Show first 3
        print(f"\n[{sig.level}] {sig.name}")
        print(sig.message)
    
    print()
    print("=== Checking trending ===")
    trending = scan_trending_tokens(["solana"])
    for sig in trending[:3]:
        print(f"\n[{sig.level}] {sig.name}")
        print(sig.message)
